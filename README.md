# MediaPipe Pose & Holistic Analysis for Multimodal Interaction Research

> Attribution: Use the script prepared for the focus group session. Attribute to this Envision Box module: [Module](https://www.envisionbox.org/embedded_MergingMultimodal_inPython.html)

A comprehensive tutorial for extracting human pose landmarks using Google's MediaPipe library, specifically designed for **multimodal interaction research** and gesture analysis.

## ğŸ”¬ Research Context

This project is based on the [MEDAL Workshop on Multimodal Interaction](https://github.com/Multimodal-Language-Department-MPI-NL/medal_workshop_on_multimodal_interaction/tree/main) by Raquel Fernandez & Esam Ghaleb. It provides tools for:

- **Gesture segmentation** and classification
- **Kinematic feature extraction** for movement analysis  
- **Multimodal similarity analysis** combining speech and gesture features
- **Temporal alignment** of different modalities in conversation

## ğŸ¯ What This Project Does

1. **Pose Landmark Extraction**: Extract 33 body landmarks using MediaPipe Pose or 75+ landmarks (body + hands + face) using MediaPipe Holistic
2. **Model Configuration**: Understand trade-offs between different model complexities (0, 1, 2) and their impact on accuracy vs. speed
3. **Temporal Smoothing**: Learn how tracking affects landmark stability and reduces jitter
4. **Gesture Analysis**: Apply pose extraction specifically for gesture segmentation and multimodal interaction research
5. **Data Visualization**: Plot landmark trajectories and analyze movement patterns

## ğŸ“ Project Structure

```
MediaPipe_keypoints/
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .gitattributes
â”œâ”€â”€ environment.yml
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ Mediapipe_Pose_Tutorial.ipynb
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ extract_mp_pose.py          # Required by the notebook
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ pose_cli.py                 # Optional CLI wrapper
â”œâ”€â”€ input_videos/
â”‚   â””â”€â”€ .gitkeep                    # Put your .mp4 files here or use webcam=0
â”œâ”€â”€ Mediapipe_results/
â”‚   â””â”€â”€ .gitkeep                    # Output directory for processed videos
â””â”€â”€ results/
    â””â”€â”€ .gitkeep                    # Output directory for keypoints data
```

## ğŸš€ Quick Start

### Run the Interactive Notebook
```bash
conda env create -f environment.yml
conda activate mp-keypoints
jupyter lab
# open notebooks/Mediapipe_Pose_Tutorial.ipynb
```



## ğŸ“Š Smoothing Process

- **Keypoints Array**: Shape `(n_frames, n_landmarks, 4)` where 4 = [x, y, z, visibility]
- **Overlay Videos**: Annotated videos with drawn landmarks saved to `Mediapipe_results/`
- **Raw Data**: Numpy arrays with landmark coordinates for further analysis

## ğŸ”§ Smoothing Techniques

## ğŸ“š Additional Resources

- **MediaPipe Documentation**: [https://mediapipe.dev/](https://mediapipe.dev/)
- **MEDAL Workshop Repository**: [https://github.com/Multimodal-Language-Department-MPI-NL/medal_workshop_on_multimodal_interaction](https://github.com/Multimodal-Language-Department-MPI-NL/medal_workshop_on_multimodal_interaction)
- **Multimodal Interaction Research**: Explore recent papers in gesture analysis and multimodal communication